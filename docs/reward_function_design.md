# Unitree Go2 速度跟踪任务奖励函数设计详解

## 任务概述

**任务名称**: `RobotLab-Isaac-Velocity-Rough-Unitree-Go2-v0`

**任务目标**: 训练Unitree Go2四足机器人在崎岖地形上进行速度跟踪，包括前后方向线速度、左右方向线速度和偏航角速度的跟踪。

**配置文件位置**: `source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity/config/quadruped/unitree_go2/rough_env_cfg.py`

---

## 奖励函数总览

该任务包含 **21个激活的奖励项**，分为以下几类：
1. **速度跟踪奖励** (2项) - 核心任务目标
2. **Base（机身）惩罚项** (2项) - 保持机身稳定
3. **关节惩罚项** (9项) - 约束关节行为
4. **动作惩罚项** (1项) - 平滑动作输出
5. **接触传感器相关** (2项) - 避免不良接触
6. **步态相关奖励** (5项) - 优化步态质量

**总奖励** = Σ(权重 × 奖励值)

---

## 详细奖励函数设计

### 一、速度跟踪奖励 (核心任务)

#### 1. `track_lin_vel_xy_exp` (权重: **3.0**)
- **设计意图**: 奖励机器人跟踪命令的线速度（x和y方向）
- **计算方式**: 使用指数核函数衡量速度误差
  ```
  error = |cmd_vel_xy - actual_vel_xy|²
  reward = exp(-error / std²) × gravity_factor
  ```
- **参数**: 
  - `std = √0.25 = 0.5` - 标准差，控制容错范围
  - `gravity_factor` - 只在机器人接近直立时给予奖励（0-0.7范围）
- **权重解释**: 3.0 是所有奖励中最高的，说明线速度跟踪是最重要的任务目标
- **实际效果**: 机器人会努力匹配指令速度，权重越大跟踪越精准

#### 2. `track_ang_vel_z_exp` (权重: **1.5**)
- **设计意图**: 奖励机器人跟踪命令的角速度（偏航方向）
- **计算方式**: 与线速度类似，使用指数核函数
  ```
  error = |cmd_ang_vel_z - actual_ang_vel_z|²
  reward = exp(-error / std²) × gravity_factor
  ```
- **权重解释**: 1.5 是线速度权重的一半，表示角速度跟踪也很重要但优先级略低于线速度
- **实际效果**: 机器人能够准确地转向，响应偏航角速度命令

---

### 二、Base（机身）惩罚项

#### 3. `lin_vel_z_l2` (权重: **-2.0**)
- **设计意图**: 惩罚机身在z轴（垂直）方向的速度
- **计算方式**: L2平方范数
  ```
  penalty = |base_vel_z|²
  ```
- **权重解释**: -2.0 是较大的惩罚，表示要严格限制垂直方向运动
- **实际效果**: 防止机器人跳跃或上下颠簸，保持平稳运动

#### 4. `ang_vel_xy_l2` (权重: **-0.05**)
- **设计意图**: 惩罚机身在roll和pitch方向的角速度
- **计算方式**: L2平方范数
  ```
  penalty = |base_ang_vel_xy|²
  ```
- **权重解释**: -0.05 是较小的惩罚，允许一定程度的姿态调整
- **实际效果**: 限制机身前后左右摇摆，保持相对稳定的姿态

---

### 三、关节惩罚项

#### 5. `joint_torques_l2` (权重: **-2.5e-5** = -0.000025)
- **设计意图**: 惩罚关节扭矩的使用，鼓励能量效率
- **计算方式**: 所有关节扭矩的L2平方和
  ```
  penalty = Σ|joint_torque|²
  ```
- **权重解释**: 非常小的惩罚系数，平衡能效和任务完成
- **实际效果**: 在完成任务的同时，尽量减少能量消耗

#### 6. `joint_acc_l2` (权重: **-2.5e-7** = -0.00000025)
- **设计意图**: 惩罚关节加速度，促进平滑运动
- **计算方式**: 关节加速度的L2平方和
  ```
  penalty = Σ|joint_acceleration|²
  ```
- **权重解释**: 极小的惩罚，轻微约束加速度
- **实际效果**: 使关节运动更加平滑，减少震动

#### 7. `joint_pos_limits` (权重: **-5.0**)
- **设计意图**: 惩罚关节位置超出软限位
- **计算方式**: 检查关节是否接近或超出限位
- **权重解释**: -5.0 是非常大的惩罚，表示严格限制关节范围
- **实际效果**: 保护机器人硬件，防止关节过度伸展或收缩

#### 8. `joint_power` (权重: **-2e-5** = -0.00002)
- **设计意图**: 惩罚关节功率消耗
- **计算方式**: 
  ```
  power = Σ|joint_vel × joint_torque|
  ```
- **权重解释**: 小惩罚，鼓励能效
- **实际效果**: 结合速度和扭矩，全面优化能量使用

#### 9. `stand_still` (权重: **-2.0**)
- **设计意图**: 当命令速度很小时，惩罚关节偏离默认位置
- **计算方式**: 仅在 |cmd| < 0.06 时激活
  ```
  penalty = Σ|joint_pos - default_pos| (当命令小时)
  ```
- **权重解释**: -2.0 的惩罚，确保静止时保持默认姿态
- **实际效果**: 机器人在收到零命令或小命令时会保持站立姿态

#### 10. `joint_pos_penalty` (权重: **-1.0**)
- **设计意图**: 运动时轻度惩罚、静止时重度惩罚关节偏离默认位置
- **计算方式**: 
  ```
  penalty = |joint_pos - default_pos|
  静止时penalty × 5.0
  ```
- **参数**:
  - `stand_still_scale = 5.0` - 静止时惩罚倍数
  - `velocity_threshold = 0.5` - 速度阈值
  - `command_threshold = 0.1` - 命令阈值
- **权重解释**: -1.0 配合倍数放大，强化静止时的姿态约束
- **实际效果**: 区分运动和静止状态，分别施加不同强度的约束

#### 11. `joint_mirror` (权重: **-0.05**)
- **设计意图**: 惩罚对称关节位置差异，促进对称步态
- **计算方式**: 
  ```
  对称对: [FR_* <-> RL_*], [FL_* <-> RR_*]
  penalty = Σ|joint_pos[FR] - joint_pos[RL]|²
  ```
- **权重解释**: -0.05 轻度惩罚，允许适度的不对称
- **实际效果**: 鼓励对角线对称的Trot步态（前右-后左，前左-后右）

#### 12. `joint_mirror` 的镜像对配置
- **对称关节对**:
  - 前右 ↔ 后左: `FR_(hip|thigh|calf)` ↔ `RL_(hip|thigh|calf)`
  - 前左 ↔ 后右: `FL_(hip|thigh|calf)` ↔ `RR_(hip|thigh|calf)`

#### 13. 未激活: `joint_vel_l2` (权重: 0)
- 关节速度惩罚被禁用，允许快速腿部运动

---

### 四、动作惩罚项

#### 14. `action_rate_l2` (权重: **-0.01**)
- **设计意图**: 惩罚动作的快速变化，促进平滑控制
- **计算方式**: 
  ```
  penalty = Σ|action[t] - action[t-1]|²
  ```
- **权重解释**: -0.01 适中惩罚，平衡响应速度和平滑度
- **实际效果**: 使策略输出更加平滑，减少高频震荡

---

### 五、接触传感器相关

#### 15. `undesired_contacts` (权重: **-1.0**)
- **设计意图**: 惩罚非足部部位（如膝盖、髋部、躯干）与地面接触
- **计算方式**: 检测非足部body的接触力
  ```
  penalty = count(contact_force > 1.0N) for non-foot bodies
  ```
- **监控部位**: 除了 `.*_foot` 外的所有body
- **权重解释**: -1.0 严格惩罚，保护硬件
- **实际效果**: 防止机器人摔倒或拖着身体走

#### 16. `contact_forces` (权重: **-1.5e-4** = -0.00015)
- **设计意图**: 轻度惩罚足部过大的接触力
- **计算方式**: 
  ```
  penalty = Σ max(0, contact_force - 100N)
  ```
- **阈值**: 100N
- **权重解释**: 极小惩罚，只限制过大冲击力
- **实际效果**: 鼓励轻柔着地，保护机器人关节

---

### 六、步态相关奖励

#### 17. `feet_air_time` (权重: **0.1**)
- **设计意图**: 奖励足部在空中停留时间超过阈值
- **计算方式**: 
  ```
  reward = Σ max(0, air_time - 0.5s) (仅在首次接地时)
  ```
- **阈值**: 0.5秒
- **激活条件**: 仅在 |cmd| > 0.1 时
- **权重解释**: 0.1 小奖励，鼓励抬腿
- **实际效果**: 促进明确的摆动相，避免拖脚

#### 18. `feet_air_time_variance` (权重: **-1.0**)
- **设计意图**: 惩罚各足空中时间的方差
- **计算方式**: 
  ```
  penalty = var(air_time) + var(contact_time)
  ```
- **权重解释**: -1.0 较大惩罚，要求步态一致性
- **实际效果**: 促进四足协调运动，避免跛行

#### 19. `feet_gait` (权重: **0.5**)
- **设计意图**: 奖励Trot步态（对角线同步）
- **计算方式**: 复杂的步态时序奖励
  - 同步对内: `FL-RR` 和 `FR-RL` 应该同时接触/离地
  - 同步对间: 两对应该交替
  ```
  reward = sync_reward × async_reward
  ```
- **参数**:
  - `std = √0.5` - 标准差
  - `max_err = 0.2s` - 最大误差容忍
  - `synced_feet_pair_names`: `(FL_foot, RR_foot)`, `(FR_foot, RL_foot)`
- **激活条件**: |cmd| > 0.1 或 |body_vel| > 0.5
- **权重解释**: 0.5 适中奖励，强化Trot步态
- **实际效果**: 产生稳定高效的对角线步态

#### 20. `feet_contact_without_cmd` (权重: **0.1**)
- **设计意图**: 奖励静止时的足部接触
- **计算方式**: 
  ```
  reward = count(contact) (仅当|cmd| < 0.1时)
  ```
- **激活条件**: 命令速度 < 0.1
- **权重解释**: 0.1 小奖励，鼓励稳定站立
- **实际效果**: 静止时保持四足着地，增加稳定性

#### 21. `feet_slide` (权重: **-0.1**)
- **设计意图**: 惩罚足部着地时的滑动
- **计算方式**: 
  ```
  penalty = Σ(foot_lateral_velocity × contact_flag)
  ```
- **权重解释**: -0.1 适中惩罚
- **实际效果**: 鼓励静摩擦接触，提高牵引力和能效

#### 22. `feet_height_body` (权重: **-5.0**)
- **设计意图**: 惩罚摆动足距离机身过近（相对高度）
- **计算方式**: 在机身坐标系中测量
  ```
  penalty = Σ(foot_z - target_z)² × velocity_factor
  当足部移动快时惩罚
  ```
- **目标高度**: -0.2m（相对机身）
- **参数**: `tanh_mult = 2.0` - 速度敏感度
- **激活条件**: |cmd| > 0.1
- **权重解释**: -5.0 很大的惩罚，严格约束
- **实际效果**: 防止足部在摆动时踢到机身或障碍物

#### 23. `upward` (权重: **1.0**)
- **设计意图**: 奖励机身保持直立
- **计算方式**: 
  ```
  reward = (1 - projected_gravity_z)²
  当projected_gravity_z = 1时reward = 0（完全直立）
  ```
- **权重解释**: 1.0 标准奖励权重
- **实际效果**: 持续鼓励机器人保持直立姿态，抵抗倾倒

---

### 七、未激活的奖励项

以下奖励项权重设置为0，已被禁用：

1. **`is_terminated`** (0) - 终止惩罚
2. **`flat_orientation_l2`** (0) - 姿态平坦度
3. **`base_height_l2`** (0) - 机身高度控制
4. **`body_lin_acc_l2`** (0) - 机身线加速度
5. **`joint_vel_l2`** (0) - 关节速度
6. **`joint_vel_limits`** (0) - 关节速度限制
7. **`feet_contact`** (0) - 特定足部接触数量
8. **`feet_stumble`** (0) - 足部绊倒检测
9. **`feet_height`** (0) - 足部绝对高度（世界坐标）

---

## 奖励权重设计原则

### 权重分级

1. **最高优先级** (|weight| ≥ 1.0):
   - `track_lin_vel_xy_exp` (3.0) - 核心任务
   - `track_ang_vel_z_exp` (1.5) - 核心任务
   - `joint_pos_limits` (-5.0) - 安全约束
   - `feet_height_body` (-5.0) - 安全约束
   - `stand_still` (-2.0) - 行为约束
   - `lin_vel_z_l2` (-2.0) - 行为约束

2. **中等优先级** (0.1 ≤ |weight| < 1.0):
   - `feet_gait` (0.5) - 步态优化
   - `feet_air_time_variance` (-1.0) - 步态协调
   - `undesired_contacts` (-1.0) - 安全
   - `joint_pos_penalty` (-1.0) - 行为约束
   - `feet_slide` (-0.1) - 效率优化

3. **低优先级** (|weight| < 0.1):
   - `feet_air_time` (0.1) - 步态细节
   - `feet_contact_without_cmd` (0.1) - 站立稳定性
   - `ang_vel_xy_l2` (-0.05) - 姿态稳定
   - `joint_mirror` (-0.05) - 对称性
   - `action_rate_l2` (-0.01) - 平滑性

4. **极低优先级** (|weight| < 0.001):
   - `joint_torques_l2` (-2.5e-5) - 能效
   - `joint_power` (-2e-5) - 能效
   - `contact_forces` (-1.5e-4) - 冲击力限制
   - `joint_acc_l2` (-2.5e-7) - 平滑性

### 设计理念

1. **任务导向**: 速度跟踪奖励权重最大，明确任务目标
2. **安全优先**: 关节限位和不良接触有高惩罚权重
3. **分层优化**: 从核心任务到细节优化，权重递减
4. **平衡权衡**: 
   - 任务完成 vs 能量效率
   - 快速响应 vs 平滑控制
   - 步态质量 vs 速度跟踪

---

## 特殊机制

### 1. Gravity Factor
大部分奖励乘以一个"重力因子"：
```python
reward *= torch.clamp(-projected_gravity_b[:, 2], 0, 0.7) / 0.7
```
- **作用**: 仅在机器人接近直立时给予奖励
- **范围**: 0到1，完全直立时为1
- **意义**: 防止机器人在倒地或翻滚时仍获得奖励

### 2. Command Gating
许多奖励仅在命令速度大于阈值时激活：
```python
reward *= torch.norm(command, dim=1) > 0.1
```
- **作用**: 区分运动和静止状态
- **意义**: 允许不同状态下的不同行为模式

### 3. Velocity Gating
部分奖励考虑实际速度：
```python
torch.logical_or(cmd > threshold, body_vel > threshold)
```
- **作用**: 即使命令小，但机器人仍在运动时也激活
- **意义**: 更鲁棒的状态判断

---

## 训练效果预期

基于这套奖励函数，训练后的策略应该能够：

1. ✅ **精准跟踪速度命令** - 最高权重保证
2. ✅ **保持稳定姿态** - 多重惩罚约束
3. ✅ **展现Trot步态** - 步态奖励引导
4. ✅ **能量效率良好** - 多个能效惩罚项
5. ✅ **动作平滑** - 动作变化率惩罚
6. ✅ **避免不良接触** - 高权重惩罚保护
7. ✅ **四足协调运动** - 方差惩罚确保
8. ✅ **静止时稳定站立** - 专门的静止奖励
9. ✅ **适应崎岖地形** - 整体设计支持

---

## 调优建议

如果训练结果不理想，可以考虑调整以下权重：

### 速度跟踪不准确
- 增大 `track_lin_vel_xy_exp` 和 `track_ang_vel_z_exp`
- 减小其他冲突的惩罚项权重

### 步态不稳定
- 增大 `feet_gait` 权重
- 增大 `feet_air_time_variance` 惩罚

### 能量消耗过大
- 增大 `joint_torques_l2` 和 `joint_power` 惩罚

### 动作不够平滑
- 增大 `action_rate_l2` 惩罚
- 增大 `joint_acc_l2` 惩罚

### 机器人容易摔倒
- 增大 `undesired_contacts` 惩罚
- 增大 `upward` 奖励
- 增大 `flat_orientation_l2` 奖励（需要激活）

---

## 总结

这套奖励函数设计体现了：
- **明确的任务目标** (速度跟踪)
- **安全的运动约束** (关节限位、接触限制)
- **高质量的步态** (Trot、协调、平滑)
- **能源效率考虑** (扭矩、功率惩罚)
- **鲁棒的行为模式** (区分运动/静止状态)

通过21个精心设计和权重平衡的奖励项，该任务能够训练出在崎岖地形上稳定、高效、精准的四足运动控制策略。
