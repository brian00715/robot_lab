# å‡å°‘è·Ÿè¸ªè¯¯å·®çš„ä¼˜åŒ–æ–¹æ¡ˆ

## å½“å‰çŠ¶æ€
- **Stage 3**: 0-15k iterations (Â±10cm height, Â±20Â° roll, Â±12Â° pitch/yaw)
- **Stage 4**: 15k+ iterations (Â±15cm height, Â±30Â° roll, Â±15Â° pitch/yaw)
- **Current Weights**:
  - `track_height_exp`: weight=2.0, std=0.5 (tolerance Â±50cm)
  - `track_orientation_exp`: weight=1.0, std=0.707 (tolerance Â±40Â°)

## ğŸ¯ æ–¹æ¡ˆä¼˜å…ˆçº§

### â­ æ–¹æ¡ˆ 1: è°ƒæ•´å¥–åŠ±æ ‡å‡†å·®ï¼ˆæœ€æœ‰æ•ˆï¼Œæ¨èä¼˜å…ˆå°è¯•ï¼‰

**åŸç†**: å‡å° `std` å‚æ•°ä¼šè®©å¥–åŠ±å‡½æ•°å¯¹è¯¯å·®æ›´æ•æ„Ÿï¼Œæƒ©ç½šæ›´ä¸¥æ ¼ã€‚

**å½“å‰é—®é¢˜**:
- Height std=0.5 â†’ 50cm è¯¯å·®æ—¶å¥–åŠ±ä»æœ‰ 37% (å¤ªå®½å®¹)
- Orientation std=0.707rad (40Â°) â†’ 40Â° è¯¯å·®æ—¶å¥–åŠ±ä»æœ‰ 37% (å¤ªå®½å®¹)

**å»ºè®®ä¿®æ”¹**:

```python
# In rough_env_cfg.py, line ~189
self.rewards.track_height_exp = RewTerm(
    func=mdp.track_height_exp,
    weight=2.0,  # Keep current weight
    params={
        "command_name": "base_velocity_pose",
        "std": math.sqrt(0.05),  # æ”¹å°! ä» 0.25 â†’ 0.05 (stdä»0.5â†’0.22m)
        # æ–°çš„å®¹å¿åº¦: Â±5cmè¯¯å·® â†’ 79%å¥–åŠ±, Â±10cm â†’ 61%, Â±22cm â†’ 37%
        "sensor_cfg": SceneEntityCfg("height_scanner_base"),
    }
)

# Line ~197
self.rewards.track_orientation_exp = RewTerm(
    func=mdp.track_orientation_exp,
    weight=1.0,  # Keep current weight
    params={
        "command_name": "base_velocity_pose",
        "std": math.sqrt(0.10),  # æ”¹å°! ä» 0.5 â†’ 0.10 (stdä»0.707â†’0.316rad â‰ˆ18Â°)
        # æ–°çš„å®¹å¿åº¦: Â±5Â°è¯¯å·® â†’ 76%å¥–åŠ±, Â±10Â° â†’ 58%, Â±18Â° â†’ 37%
    }
)
```

**é¢„æœŸæ•ˆæœ**:
- Height è·Ÿè¸ªè¯¯å·®ä» Â±10-20cm é™åˆ° Â±2-5cm
- Orientation è·Ÿè¸ªè¯¯å·®ä» Â±10-15Â° é™åˆ° Â±3-8Â°

**æµ‹è¯•æ­¥éª¤**:
```bash
# 1. ä¿®æ”¹ rough_env_cfg.py
# 2. é‡æ–°è®­ç»ƒ (--resume ç»§ç»­)
python scripts/reinforcement_learning/rsl_rl/train.py \
    --task=RobotLab-Isaac-VelocityPose-Flat-Unitree-Go2-v0 \
    --num_envs=4096 \
    --max_iterations=50000 \
    --resume \
    --load_run=2026-01-15_15-09-03 \
    --checkpoint=model_37000.pt \
    --headless

# 3. è§‚å¯Ÿ TensorBoard ä¸­çš„ track_height_exp å’Œ track_orientation_exp å¥–åŠ±å˜åŒ–
```

---

### â­â­ æ–¹æ¡ˆ 2: å¢åŠ å¥–åŠ±æƒé‡ï¼ˆç®€å•ä½†å¯èƒ½ä¸å¤Ÿç²¾ç»†ï¼‰

**åŸç†**: å¢åŠ æƒé‡ä¼šè®©æ™ºèƒ½ä½“æ›´é‡è§†è¿™äº›å¥–åŠ±é¡¹ã€‚

**å»ºè®®ä¿®æ”¹**:
```python
# Height tracking
self.rewards.track_height_exp.weight = 4.0  # ä» 2.0 â†’ 4.0 (2å€)

# Orientation tracking  
self.rewards.track_orientation_exp.weight = 2.0  # ä» 1.0 â†’ 2.0 (2å€)
```

**æ³¨æ„äº‹é¡¹**:
- æƒé‡å¤ªé«˜å¯èƒ½å¯¼è‡´å…¶ä»–è¡Œä¸ºï¼ˆå¦‚æ­¥æ€ã€ç¨³å®šæ€§ï¼‰å˜å·®
- å»ºè®®å…ˆå°è¯•æ–¹æ¡ˆ1ï¼ˆè°ƒæ•´stdï¼‰ï¼Œå¦‚æœæ•ˆæœä¸å¤Ÿå†å¢åŠ æƒé‡

---

### â­â­â­ æ–¹æ¡ˆ 3: Stage-based æ¸è¿›å¼å¥–åŠ±å‚æ•°ï¼ˆæœ€ç²¾ç»†ï¼‰

**åŸç†**: åœ¨ä¸åŒ Stage ä½¿ç”¨ä¸åŒçš„å¥–åŠ±å‚æ•°ï¼Œé€æ­¥æ”¶ç´§å®¹å¿åº¦ã€‚

**å®ç°**: åœ¨ `curriculums.py` ä¸­æ·»åŠ å¥–åŠ±å‚æ•°è°ƒæ•´é€»è¾‘

```python
# In command_curriculum_height_pose function, after setting ranges

# Also adjust reward tolerance based on stage
if hasattr(env, "reward_manager"):
    # Get reward terms
    height_reward_cfg = env.reward_manager.get_term_cfg("track_height_exp")
    orient_reward_cfg = env.reward_manager.get_term_cfg("track_orientation_exp")
    
    if target_stage == 3:  # Medium range, medium tolerance
        height_reward_cfg.params["std"] = math.sqrt(0.08)  # stdâ‰ˆ0.28m (Â±8cm â†’ 70% reward)
        orient_reward_cfg.params["std"] = math.sqrt(0.15)  # stdâ‰ˆ0.39rad (Â±22Â° â†’ 70% reward)
    elif target_stage == 4:  # Maximum range, strict tolerance
        height_reward_cfg.params["std"] = math.sqrt(0.05)  # stdâ‰ˆ0.22m (Â±5cm â†’ 78% reward)
        orient_reward_cfg.params["std"] = math.sqrt(0.10)  # stdâ‰ˆ0.32rad (Â±18Â° â†’ 70% reward)
```

---

### æ–¹æ¡ˆ 4: æ·»åŠ è¯¯å·®æƒ©ç½šé¡¹ï¼ˆè¡¥å……æ–¹æ¡ˆï¼‰

**åŸç†**: å¯¹å¤§è¯¯å·®é¢å¤–æƒ©ç½šï¼Œè¿«ä½¿ç­–ç•¥æ›´ä¿å®ˆã€‚

**å®ç°**: åœ¨ `rewards.py` ä¸­æ·»åŠ æ–°çš„æƒ©ç½šé¡¹

```python
def height_error_penalty(
    env: ManagerBasedRLEnv,
    command_name: str,
    threshold: float = 0.05,  # 5cm threshold
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """Penalty for height tracking error exceeding threshold."""
    asset: RigidObject = env.scene[asset_cfg.name]
    command = env.command_manager.get_command(command_name)
    target_height = command[:, 3]
    current_height = asset.data.root_pos_w[:, 2] - env.scene.env_origins[:, 2]
    error = torch.abs(current_height - target_height)
    # Only penalize if error > threshold
    penalty = torch.clamp(error - threshold, min=0.0)
    return -penalty  # Negative reward

def orientation_error_penalty(
    env: ManagerBasedRLEnv,
    command_name: str,
    threshold: float = 0.174,  # 10Â° threshold (0.174 rad)
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """Penalty for orientation tracking error exceeding threshold."""
    from isaaclab.utils.math import quat_error_magnitude
    asset: RigidObject = env.scene[asset_cfg.name]
    command = env.command_manager.get_command(command_name)
    # ... (similar to track_orientation_exp but with threshold)
    angle_error = ...  # Calculate angle error
    penalty = torch.clamp(angle_error - threshold, min=0.0)
    return -penalty
```

**é…ç½®**:
```python
# In rough_env_cfg.py
self.rewards.height_error_penalty = RewTerm(
    func=mdp.height_error_penalty,
    weight=1.0,
    params={"command_name": "base_velocity_pose", "threshold": 0.05}
)
self.rewards.orientation_error_penalty = RewTerm(
    func=mdp.orientation_error_penalty,
    weight=0.5,
    params={"command_name": "base_velocity_pose", "threshold": 0.174}
)
```

---

### æ–¹æ¡ˆ 5: è°ƒæ•´å…¶ä»–ç›¸å…³å‚æ•°

#### 5.1 å¢åŠ åŠ é€Ÿåº¦å¹³æ»‘æƒ©ç½š

```python
# Make height/orientation changes smoother
self.rewards.base_lin_acc_z_l2.weight = -0.05  # ä» -0.02 å¢åŠ åˆ° -0.05
self.rewards.base_ang_acc_xy_l2.weight = -0.02  # ä» -0.01 å¢åŠ åˆ° -0.02
```

#### 5.2 è°ƒæ•´ PD æ§åˆ¶å¢ç›Šï¼ˆå¦‚æœä½¿ç”¨ PD controllerï¼‰

```python
# In actuator config (if using PD controller)
# Increase stiffness/damping for more responsive tracking
stiffness: 50.0  # Increase from default
damping: 2.0     # Increase proportionally
```

#### 5.3 å‡å°å‘½ä»¤é‡é‡‡æ ·æ—¶é—´

```python
# In velocity_pose_env_cfg.py
base_velocity_pose = mdp.UniformVelocityPoseCommandCfg(
    resampling_time_range=(5.0, 8.0),  # ä» (10.0, 10.0) æ”¹ä¸ºæ›´çŸ­
    # æ›´é¢‘ç¹çš„å‘½ä»¤å˜åŒ–ä¼šè¿«ä½¿ç­–ç•¥æ›´å¿«å“åº”
)
```

---

## ğŸ¯ æ¨èå®æ–½é¡ºåº

### Phase 1: å¿«é€ŸéªŒè¯ï¼ˆ1-2å°æ—¶è®­ç»ƒï¼‰
1. **å®æ–½æ–¹æ¡ˆ1**: è°ƒæ•´ std å‚æ•°
   - Height std: 0.5 â†’ 0.22 (æ”¹å° `math.sqrt(0.05)`)
   - Orientation std: 0.707 â†’ 0.316 (æ”¹å° `math.sqrt(0.10)`)
2. ç”¨ `--resume` ç»§ç»­è®­ç»ƒ 5000 iterations
3. è§‚å¯Ÿ TensorBoard:
   - `Rewards/track_height_exp` åº”è¯¥**ä¸‹é™**ï¼ˆå› ä¸ºæ ‡å‡†æ›´ä¸¥æ ¼ï¼‰
   - `Rewards/track_orientation_exp` åº”è¯¥**ä¸‹é™**
   - ä½†æœ€ç»ˆä¼šé€æ¸**å›å‡**ï¼ˆç­–ç•¥é€‚åº”æ–°æ ‡å‡†ï¼‰

### Phase 2: å¦‚æœæ•ˆæœä¸å¤Ÿï¼ˆå†è®­ç»ƒ2-3å°æ—¶ï¼‰
4. **å åŠ æ–¹æ¡ˆ2**: å¢åŠ æƒé‡
   - Height weight: 2.0 â†’ 3.0
   - Orientation weight: 1.0 â†’ 1.5
5. ç»§ç»­è®­ç»ƒ 5000 iterations

### Phase 3: ç²¾ç»†è°ƒä¼˜ï¼ˆå¯é€‰ï¼‰
6. **å®æ–½æ–¹æ¡ˆ3**: Stage-based å‚æ•°
7. **å®æ–½æ–¹æ¡ˆ4**: æ·»åŠ è¯¯å·®æƒ©ç½šé¡¹ï¼ˆå¦‚æœè¿˜ä¸å¤Ÿä¸¥æ ¼ï¼‰

---

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

åœ¨ TensorBoard ä¸­é‡ç‚¹è§‚å¯Ÿï¼š

### å…³é”®å¥–åŠ±æŒ‡æ ‡
- `Rewards/track_height_exp`: åº”è¯¥ä»ä½è°·é€æ¸å›å‡åˆ° 0.8-0.95
- `Rewards/track_orientation_exp`: åº”è¯¥ä»ä½è°·é€æ¸å›å‡åˆ° 0.7-0.9
- `Rewards/total`: æ€»å¥–åŠ±å¯èƒ½æš‚æ—¶ä¸‹é™ï¼Œä½†ä¼šæ¢å¤

### è°ƒè¯•ä¿¡æ¯ï¼ˆTerminalè¾“å‡ºï¼‰
```
[DEBUG] Orientation Tracking Reward Statistics:
  Quaternion error angle (deg): mean=5.0, max=15.0  â† ç›®æ ‡: mean<5Â°, max<12Â°
  Final reward: mean=0.85  â† ç›®æ ‡: mean>0.80

[DEBUG] Height Tracking Reward Statistics:
  Height error (m): mean=0.02, max=0.08  â† ç›®æ ‡: mean<0.03, max<0.06
  Final reward: mean=0.90  â† ç›®æ ‡: mean>0.85
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä¸è¦ä¸€æ¬¡æ”¹å¤ªå¤š**: å…ˆè¯•æ–¹æ¡ˆ1ï¼Œè§‚å¯Ÿæ•ˆæœå†å åŠ å…¶ä»–æ–¹æ¡ˆ
2. **ç›‘æ§å‰¯ä½œç”¨**: 
   - æ­¥æ€è´¨é‡ (`feet_air_time`, `feet_contact`)
   - ç¨³å®šæ€§ (`base_height`, `base_orientation`)
   - èƒ½è€— (`action_rate`, `joint_torques`)
3. **ä¿å­˜checkpoint**: æ¯æ¬¡ä¿®æ”¹å‰ä¿å­˜å¥½çš„ checkpoint
4. **å¯¹æ¯”æµ‹è¯•**: ç”¨ play.py å¯¹æ¯”ä¿®æ”¹å‰åçš„å®é™…è¡¨ç°

---

## ğŸ“ éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶

### æ–¹æ¡ˆ1ï¼ˆæ¨èå…ˆè¯•ï¼‰:
- `source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity_pose/config/quadruped/unitree_go2/rough_env_cfg.py`
  * Line ~189: `track_height_exp` çš„ `std` å‚æ•°
  * Line ~197: `track_orientation_exp` çš„ `std` å‚æ•°

### æ–¹æ¡ˆ2ï¼ˆå åŠ ï¼‰:
- åŒä¸Šæ–‡ä»¶
  * Line ~186: `track_height_exp.weight`
  * Line ~196: `track_orientation_exp.weight`

### æ–¹æ¡ˆ3ï¼ˆè¿›é˜¶ï¼‰:
- `source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity_pose/mdp/curriculums.py`
  * åœ¨ `command_curriculum_height_pose` å‡½æ•°ä¸­æ·»åŠ å¥–åŠ±å‚æ•°è°ƒæ•´é€»è¾‘

### æ–¹æ¡ˆ4ï¼ˆè¡¥å……ï¼‰:
- `source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity_pose/mdp/rewards.py`
  * æ·»åŠ æ–°çš„æƒ©ç½šå‡½æ•°
- `rough_env_cfg.py`
  * æ³¨å†Œæ–°çš„å¥–åŠ±é¡¹

---

## ğŸš€ ç«‹å³å¯ç”¨çš„ä¿®æ”¹å‘½ä»¤

```bash
# 1. å¤‡ä»½å½“å‰é…ç½®
cp source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity_pose/config/quadruped/unitree_go2/rough_env_cfg.py \
   source/robot_lab/robot_lab/tasks/manager_based/locomotion/velocity_pose/config/quadruped/unitree_go2/rough_env_cfg.py.backup

# 2. ä½¿ç”¨ Copilot ä¿®æ”¹ rough_env_cfg.py ä¸­çš„ std å‚æ•°ï¼ˆæ–¹æ¡ˆ1ï¼‰

# 3. ç»§ç»­è®­ç»ƒ
python scripts/reinforcement_learning/rsl_rl/train.py \
    --task=RobotLab-Isaac-VelocityPose-Flat-Unitree-Go2-v0 \
    --num_envs=4096 \
    --max_iterations=50000 \
    --resume \
    --load_run=2026-01-15_15-09-03 \
    --checkpoint=model_37000.pt \
    --headless
```

---

*Created: 2026-01-15*
*For: VelocityPose Stage 3/4 Training Optimization*
